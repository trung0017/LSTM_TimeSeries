# -*- coding: utf-8 -*-
"""Bản sao của NienLuanCoSo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o7Ffwi2LNakuslltbig1Y1VwUCtgW-wE
"""

!python --version

## Kiểm tra GPU
!nvidia-smi

"""Import các thư viện"""

# Commented out IPython magic to ensure Python compatibility.
import torch

import copy
import numpy as np
import pandas as pd
import seaborn as sns 
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from torch import nn, optim
import torch.nn.functional as F

# Dùng vẽ các biểu đồ
# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

"""Hàm dùng để đọc dữ liệu"""

import pandas as pd
from pandas import DataFrame
from scipy.io import arff
import numpy as np


def load(arff_file,decode_str=True):
    """Reads an ARFF file via scipy and returns a pandas DataFrame.
    Note: same as scipy.io.arff.load, this function does not support ARFF string types.
    
    Parameters
    ----------
    arff_file : str
        Path to ARFF file.
    decode_str : bool
        Wether or not sanitize resulting strings (removes the b'')
        
    Returns
    -------
    pandas.DataFrame
        Columns are ARFF attributes, rows correspond to instances.
    """
    data, meta = arff.loadarff(arff_file) 
    df = DataFrame(data,columns = meta.names())
    if decode_str:        
        df_str = df.select_dtypes(include=['object'])        
        if not df_str.empty:
            df[df_str.columns] = df_str.applymap(lambda x:x.decode('utf-8'))      
    return df


def _get_arff_meta_dict(dataframe):
    """Generates a dictionary with the ARFF metadata from a pandas DataFrame.    
    
    Parameters
    ----------
    dataframe : pandas.DataFrame
        DataFrame returned by `arff2pandas`.    
        
    Returns
    -------
    dict
        Keys: `_attributes`: attribute list; `_n`: number of instances. Left keys are the attributes properties.
    """    
    meta = {'_attributes':list(dataframe.columns)}
    meta['_n'] = dataframe.shape[0]
    for c in dataframe.columns:
        #print(c,df[c].dtype)
        meta[c] = {'dtype':str(dataframe[c].dtype)}
        if dataframe[c].dtype == 'object':
            meta[c]['type'] = 'nominal'
            meta[c]['values'] = list(dataframe[c].unique())
            meta[c]['min'] = np.NaN
            meta[c]['max'] = np.NaN
            meta[c]['mean'] = np.NaN
            meta[c]['std'] = np.NaN
        elif dataframe[c].dtype  == 'float64':            
            meta[c]['type'] = 'numeric'
            meta[c]['values'] = len(dataframe[c].unique())
            meta[c]['min'] = dataframe[c].min()
            meta[c]['max'] = dataframe[c].max()
            meta[c]['mean'] = dataframe[c].mean()
            meta[c]['std'] = dataframe[c].std()
        elif str(dataframe[c].dtype).startswith('date'):            
            meta[c]['type'] = 'date'
            meta[c]['values'] = len(dataframe[c].unique())
            meta[c]['min'] = dataframe[c].min()
            meta[c]['max'] = dataframe[c].max()
            meta[c]['mean'] = np.NaN
            meta[c]['std'] = np.NaN
    return meta


def get_meta(dataframe):
    """Generates a DataFrame with from the ARFF metadata from a pandas DataFrame.    
    
    Parameters
    ----------
    dataframe : pandas.DataFrame
        DataFrame returned by `arff2pandas`.    
        
    Returns
    -------
    DataFrame
        Rows are attributes, columns are properties.
    """    
    meta = _get_arff_meta_dict(dataframe)
    meta.pop('_n',None)
    attr_order = meta.pop('_attributes',None)
    _df = DataFrame.from_dict(meta,orient='index')
    _df = _df.reindex(attr_order)
    return _df

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Dữ liệu dùng để đào tạo
with open('/content/drive/MyDrive/HK2_3/CT271_NienLuanCoSo/ECG5000/ECG5000_TRAIN.arff') as f:
  train = load(f)
with open('/content/drive/MyDrive/HK2_3/CT271_NienLuanCoSo/ECG5000/ECG5000_TEST.arff') as f:
  test = load(f)
# Nối dữ liệu train và test với nhau 
df = train.append(test)
df = df.sample(frac=1.0)

df.shape

df.head()

# Đặt lại target
CLASS_NORMAL = 1
class_names = ['Normal','R on T','PVC','SP','UB']

"""1. Normal (N)
2. R-on-T Premature Ventricular Contraction (R-on-T PVC)
3. Premature Ventricular Contraction (PVC)
4. Supra-ventricular Premature or Ectopic Beat (SP or EB)
5. Unclassified Beat (UB).

Data preprocessing
"""

new_columns = list(df.columns)
new_columns[-1] = 'target'
df.columns = new_columns

"""Exploration"""

# Vẽ biểu đồ thể hiện sự mất cân bằng dữ liệu
ax = sns.countplot(df.target)
ax.set_xticklabels(class_names)

def plot_time_series_class(data, class_name, ax, n_steps=10):
  time_series_df = pd.DataFrame(data)

  smooth_path = time_series_df.rolling(n_steps).mean()
  path_deviation = 2 * time_series_df.rolling(n_steps).std()

  under_line = (smooth_path - path_deviation)[0]
  over_line = (smooth_path + path_deviation)[0]

  ax.plot(smooth_path, linewidth=2)
  ax.fill_between(
    path_deviation.index,
    under_line,
    over_line,
    alpha=.125
  )
  ax.set_title(class_name)

# Class normal có hình dạng khác biệt lớn so với các lớp khác, từ đó hy vọng mô hình có thể phát hiện được bất thường
classes = df.target.unique()

fig, axs = plt.subplots(
  nrows=len(classes) // 3 + 1,
  ncols=3,
  sharey=True,
  figsize=(14, 8)
)

for i, cls in enumerate(classes):
  ax = axs.flat[i]
  data = df[df.target == cls] \
    .drop(labels='target', axis=1) \
    .mean(axis=0) \
    .to_numpy()
  plot_time_series_class(data, class_names[i], ax)

fig.delaxes(axs.flat[-1])
fig.tight_layout();

"""Data Preprocessing"""

normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)
normal_df.shape

anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)
anomaly_df.shape

"""
Phân chia tập dữ liệu"""

train_df, val_df = train_test_split(
  normal_df,
  test_size=0.3,
  random_state=RANDOM_SEED
)

val_df, test_df = train_test_split(
  val_df,
  test_size=0.33, 
  random_state=RANDOM_SEED
)
print(train_df.shape)
print(val_df.shape)
print(test_df.shape)

def create_dataset(df):
  sequences = df.astype(np.float32).to_numpy().tolist()
  dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences]
  n_seq, seq_len, n_features = torch.stack(dataset).shape
  return dataset, seq_len, n_features

train_dataset, seq_len, n_features = create_dataset(train_df)
val_dataset, _, _ = create_dataset(val_df)
test_normal_dataset, _, _ = create_dataset(test_df)
test_anomaly_dataset, _, _ = create_dataset(anomaly_df)

print(len(train_dataset))
print(len(val_dataset))
print(len(test_normal_dataset))
print(len(test_anomaly_dataset))

"""LSTM Autoencoder"""

class Encoder(nn.Module):
  def __init__(self, seq_len, n_features, embedding_dim=64):
    super(Encoder, self).__init__()
    self.seq_len, self.n_features = seq_len, n_features
    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim

    self.rnn1 = nn.LSTM(
      input_size=n_features,
      hidden_size=self.hidden_dim,
      num_layers=1,
      batch_first=True
    )
    
    self.rnn2 = nn.LSTM(
      input_size=self.hidden_dim,
      hidden_size=embedding_dim,
      num_layers=1,
      batch_first=True
    )

  def forward(self, x):
    x = x.reshape((1, self.seq_len, self.n_features))
    x, (_, _) = self.rnn1(x)
    x, (hidden_n, _) = self.rnn2(x)

    return hidden_n.reshape((self.n_features, self.embedding_dim))

class Decoder(nn.Module):
  def __init__(self, seq_len, input_dim=64, n_features=1):
    super(Decoder, self).__init__()
    self.seq_len, self.input_dim = seq_len, input_dim
    self.hidden_dim, self.n_features = 2 * input_dim, n_features

    self.rnn1 = nn.LSTM(
      input_size=input_dim,
      hidden_size=input_dim,
      num_layers=1,
      batch_first=True
    )

    self.rnn2 = nn.LSTM(
      input_size=input_dim,
      hidden_size=self.hidden_dim,
      num_layers=1,
      batch_first=True
    )
    self.output_layer = nn.Linear(self.hidden_dim, n_features)

  def forward(self, x):
    x = x.repeat(self.seq_len, self.n_features)
    x = x.reshape((self.n_features, self.seq_len, self.input_dim))
    x, (hidden_n, cell_n) = self.rnn1(x)
    x, (hidden_n, cell_n) = self.rnn2(x)
    x = x.reshape((self.seq_len, self.hidden_dim))

    return self.output_layer(x)

class RecurrentAutoencoder(nn.Module):

  def __init__(self, seq_len, n_features, embedding_dim=64):
    super(RecurrentAutoencoder, self).__init__()

    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)
    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)

  def forward(self, x):
    x = self.encoder(x)
    x = self.decoder(x)

    return x

model = RecurrentAutoencoder(seq_len, n_features, 128)
model = model.to(device)

## Training sử dụng mất mác để kiểm tra mô hính
def train_model(model, train_dataset, val_dataset, n_epochs):
  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) ## Tối ưu hóa
  criterion = nn.L1Loss(reduction='sum').to(device) ##Tiêu chuẩn đánh giá là lỗi tuyệt đối trung bình MAE tính toán trên GPU
  history = dict(train=[], val=[])

  best_model_wts = copy.deepcopy(model.state_dict())
  best_loss = 10000.0
  
  for epoch in range(1, n_epochs + 1):
    model = model.train()

    train_losses = [] 
    for seq_true in train_dataset: ## Lặp lại từng chuỗi
      optimizer.zero_grad() ## Chuyển độ dốc về 0

      seq_true = seq_true.to(device) ## Đưa chuỗi vào GPU
      seq_pred = model(seq_true) ## Dự đoán

      loss = criterion(seq_pred, seq_true) ## Đo lỗi cho dự đoán

      loss.backward() ## Lan truyền ngược
      optimizer.step() ## Tối ưu hóa

      train_losses.append(loss.item()) ## Thêm vào lỗi trong quá trình đào tạo

    val_losses = []
    model = model.eval()
    with torch.no_grad():
      for seq_true in val_dataset:

        seq_true = seq_true.to(device)
        seq_pred = model(seq_true)

        loss = criterion(seq_pred, seq_true)
        val_losses.append(loss.item())

    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_losses)

    history['train'].append(train_loss) ## Ghi nhận lại lịch sử
    history['val'].append(val_loss)

    if val_loss < best_loss:
      best_loss = val_loss
      best_model_wts = copy.deepcopy(model.state_dict())

    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')

  model.load_state_dict(best_model_wts)
  return model.eval(), history

'''model, history = train_model(
  model, 
  train_dataset, 
  val_dataset, 
  n_epochs=15
)'''

!gdown --id 1jEYx5wGsb7Ix8cZAw3l5p5pOwHs3_I9A
model = torch.load('model.pth')
model = model.to(device)

MODEL_PATH = 'model.pth'
torch.save(model, MODEL_PATH)

def predict(model, dataset):
  predictions, losses = [], []
  criterion = nn.L1Loss(reduction='sum').to(device) ## Tính toán lỗi
  with torch.no_grad():
    model = model.eval()
    for seq_true in dataset:
      seq_true = seq_true.to(device)
      seq_pred = model(seq_true)

      loss = criterion(seq_pred, seq_true) ## Lỗi mất mác

      predictions.append(seq_pred.cpu().numpy().flatten())
      losses.append(loss.item())
  return predictions, losses

predictions, pred_losses = predict(model, test_normal_dataset)
sns.displot(pred_losses, kde=True);

predictions, pred_losses = predict(model, test_anomaly_dataset[:290])
sns.displot(pred_losses, kde=True);

len(test_anomaly_dataset[len(test_normal_dataset):])

anomaly_dataset = test_anomaly_dataset[len(test_normal_dataset):]
predictions, pred_losses = predict(model, test_anomaly_dataset[len(test_normal_dataset):])
list_losses = []
list_threshold = []
for THRESHOLD in range(20, 80):
  correct = sum(l > THRESHOLD for l in pred_losses)
  print(f'Correct anomaly predictions: {correct}/{len(anomaly_dataset)}')
  list_losses.append(np.around(correct/len(anomaly_dataset)*100))
  list_threshold.append(THRESHOLD)

import matplotlib.pyplot as plt

plt.plot(list_threshold, list_losses)

def plot_prediction(data, model, title, ax):
  predictions, pred_losses = predict(model, [data])

  ax.plot(data, label='true')
  ax.plot(predictions[0], label='reconstructed')
  ax.set_title(f'{title} (loss: {np.around(pred_losses[0], 2)})')
  ax.legend()

fig, axs = plt.subplots(
  nrows=2,
  ncols=6,
  sharey=True,
  sharex=True,
  figsize=(22, 8)
)

for i, data in enumerate(test_normal_dataset[:6]):
  plot_prediction(data, model, title='Normal', ax=axs[0, i])

for i, data in enumerate(test_anomaly_dataset[:6]):
  plot_prediction(data, model, title='Anomaly', ax=axs[1, i])

fig.tight_layout();